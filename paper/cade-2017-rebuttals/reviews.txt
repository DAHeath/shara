----------------------- REVIEW 1 ---------------------
PAPER: 83
TITLE: Solving Constrained Horn Clauses Using Dependence-Disjoint Expansions
AUTHORS: Qi Zhou and William Harris

Overall evaluation: -3 (strong reject)

----------- Overall evaluation -----------
# Summary

The paper presents an algorithm called Shara for solving non-linear
recursion-free Constrained Horn Clauses (CHC). CHC has recently
regained popularity as a means to encode verification conditions for
verification of sequential and concurrent programs. Solving
non-recursive CHC is often a step used by iterative solvers for
arbitrary CHC.

The paper presents a new algorithm to deal with an exponential
explosion of transforming non-linear CHC to linear (i.e., function
inlining). While the presented algorithm is still worst case
exponential, the authors report that it performs well in practice. The
algorithm is implemented along Duality CHC engine of Z3 and is shown
to outperform Duality on some benchmarks select from SV-COMP 2013 and
2015.

# Evaluation

I am fairly confident in the soundness of the presented
algorithm. Empirical results show that the algorithm outperforms
Duality, which is interesting. However, lack of comparison with other
tools, especially tools participated in SV-COMP, makes it hard to
judge the significance of the comparison.

However, the papers seems to miss a crucial detail. While inlining
(whether naive, DAG-, or minimal) causes exponential blowup, the worst
case-complexity of the underlying problem is not exponential!  In
particular, reachability in Boolean Programs (or Push-Down Systems) is
polynomial (e.g., [BR01] among many others). The algorithm would still
be interesting if the worst-case exponential behavior would not occur
always. This, however, is not the case. The first step --
transformation to CDD -- is done by inlining (more precisely by
cloning functions so that a call-site determines the function being
called), hence the worst-case explosion is unavoidable. For example,
the algorithm is guaranteed to achieve the worst-case on examples in
[BR01]. This severely undermines the authors' argument that they solve
non-linear systems. In fact, the algorithm only applies to non-linear
systems that can also be solved effectively by inlining. Thus, the
methods for analyzing non-linear non-recursive Horn clauses become
relevant and should be compared against.

While the work of [BR01] applies to Boolean Programs, similar
approaches exist for CHC over theories, as for example, implemented in
ARMC and Eldarica. More recently, SMT-based algorithms that use
interpolation while either not always achieving worst case [HB12], or
guaranteeing polynomial upper bound [KGS14] have been developed. Shaha
is guaranteed not to scale on the examples in the above papers since
they exhibit exponential grows during inlining.

Surprisingly, the closely related work [HB12,KGS14] is not mentioned
at all. This is especially strange since HB12 is implemented in Z3
along Duality, and [KGS14] is available as a public fork of Z3.

In summary, the paper needs to be significantly updated to include a
detailed comparison and evaluation with closely related work. In
particular, it needs to justify why a worst-case exponential algorithm
is needed when a polynomial algorithm is available and compare with
other inlining-based methods when inlining is not exponential.

[BR01] Thomas Ball, Sriram K. Rajamani: Bebop: a path-sensitive
interprocedural dataflow engine. PASTE 2001: 97-103

[HB12] Krystof Hoder, Nikolaj Bjørner: Generalized Property Directed
Reachability. SAT 2012: 157-171

[KGS14] Anvesh Komuravelli, Arie Gurfinkel, Sagar Chaki: SMT-Based
Model Checking for Recursive Programs. CAV 2014: 17-34

# Additional comments

Is the tool publicly available and experiments reproducible?

Are the benchmarks publicly available? It is not clear what was
selected.

How are the benchmarks converted to non-linear CHC? SV-COMP uses C as
the language of benchmarks. Transitioning from C to CHC is non-trivial
and can significantly affect the results (for example if inlining is
used during this transformation)

I find all the uses of reference [2] in the paper to be out of
place. [2] describes how to reduce solving quantified CHC to
quantifier-free CHC. It uses 3 different CHC solvers as a black box
(ARMC, Duality, and GPDR). However, it itself does not describe how
CHC are to be solved. Yet, the reference is used throughout to justify
claims on how existing CHC techniques operate.

The claim that existing CHC solvers enumerate all derivations is not
true for ARMC, Eldarica, and Spacer.

It is not clear what is the semantics used for uninterpreted functions
when they are used as constraints in CHC. Does a solution provide an
interpretation for those functions (as, for examples, in SMT). Or,
should the solution be valid for all interpretations? That is,
uninterpreted functions are universally quantified?

on page 8, second paragraph of Section 4. 'Solved efficiently' is
misleading because it refers to solved in linear time in the size of
an exponentially larger system.

Algorithm 3: If IsSat() on line 10 returns UNSAT, the solution to the
CHC can be extracted directly from the unsatisfiability proof. It is
not clear why additional solving is necessary. 

See for example:
Aws Albarghouthi, Arie Gurfinkel, Marsha Chechik: Whale: An
Interpolation-Based Algorithm for Inter-procedural Verification. VMCAI
2012: 39-55

The call to VC() and the call to IsSat() on line 10 are the
bottlenecks when inlining causes exponential blowup. I don't see how
the algorithm does anything to avoid the blowup.

Computing the solution to CHC by extra interpolation queries is also
done in [8]. I don't see what is the significant difference between
the Shaha algorithm and [8] in this step.

Algorithm 3, line 3: I can't understand the definition. Initially,
\sigma is empty, so it looks like no R will be selected, unless R has
no dependencies (since dependencies of R must be in Dom(\sigma), but
it is empty).

Eldarica is known to be slower than other engines, but it is optimized
to find solutions that better generalize to recursive CHC. It seems
strange to compare it with other tools on speed alone. I believe in
this benchmarks, the fastest solution is to inline and aggressively
simplify/optimize before calling IsSat(). A solution that is close to
that strategy would be faster, but would not necessarily produce the
best solution for generalization.

----------------------- REVIEW 2 ---------------------
PAPER: 83
TITLE: Solving Constrained Horn Clauses Using Dependence-Disjoint Expansions
AUTHORS: Qi Zhou and William Harris

Overall evaluation: -1 (weak reject)

----------- Overall evaluation -----------
Summary:
--------------
This paper presents a technique for solving recursion-free Horn clauses using minimal expansions. Recursion-free Horn clauses are an interesting class of Horn clauses since they correspond to hierarchical programs and also appear as sub-problems in the general problem of solving recursive programs.
There is a solving algorithm in the paper called Shara that solves a class of recursion-free Horn clauses called Clause-Dependent Disjoint clauses.

Comments:
---------------
- I am not sure how the new fragment of recursion-free Horn clauses in this paper compares to the previous notions, especially since the paper lacks a good comparison with a related paper [1] that formulates the notion of “body-disjoint” recursion-free Horn clauses. The Clause-Dependent Disjoint clauses as introduced in this paper, look to be very similar to the notion of body-disjoint Horn clauses.
The concept of Body disjoint Horn clauses also requires the relational predicates in the bodies of the clauses to be disjoint from each other and it also does not restrict the clauses to be linear.
I wonder how exactly these two notions compare to each other, and if one of them subsumes another.

- The authors claim to significantly outperform the previous tools. I think such huge improvements can be justified by a complexity argument about the algorithm. Given an underlying logic such as Presburger arithmetic, what is the worst-case complexity of solving the class of CDD Horn clauses? Does the expansion algorithm change the complexity of solving clauses? Is there any meaningful relation between the time complexities of a minimal expansion vs. an extension which is not minimal? I expect to see some arguments in those lines, especially since this paper makes a strong argument about the efficiency of the approach w.r.t all the state of the art tools.

Minor Comments:
------------------------
- The labeling of the clauses in the running example and the text have discrepancies at some points. For example, quoting from page 4 “disjunction of solutions for L3 for each clause with head L4 (namely Clause (3) and Clause (4))”. Clauses 3 and 4 do not have L4 as their head.
- Is there any specific reason to call the functions “procedures” in the pseudo-code although they compute and return values?
- The paper cites [16] and claims that during disjunctive interpolation individual derivations are solved independently and finally get combined. This is exactly what disjunctive interpolation tries to avoid by simultaneously handling several program paths together. 


[1] Philipp Ruemmer et al. “Classifying and Solving Horn Clauses for Verification”, VSTTE 2013.

----------------------- REVIEW 3 ---------------------
PAPER: 83
TITLE: Solving Constrained Horn Clauses Using Dependence-Disjoint Expansions
AUTHORS: Qi Zhou and William Harris

Overall evaluation: -2 (reject)

----------- Overall evaluation -----------
The authors of this paper present a new algorithm for solving
recursion-free constrained Horn clauses (CHCs) over various background
theories. The main idea of the paper is to identify a fragment of
recursion-free clauses, called "clause-dependent disjoint" (CDD)
clauses, and to define a bottom-up algorithm for constructing
solutions of CDD clauses by repeated binary Craig
interpolation. Arbitrary systems of recursion-free clauses can be
mapped to CDD by duplicating clauses, which in general leads to an
exponentially larger set of clauses, but seems to behave well in many
practical cases. In experiments on recursion-free clauses extracted
from SV-COMP problems, it was found that the new procedure outperforms
existing solvers for recursion-free Horn clauses in terms of runtime.

Evaluation:

It seems to me that this paper mainly reproduces existing and known
techniques for solving Horn clauses, originality is unclear. The
notion of "clause-dependent disjoint" clauses coincides with the class
of "body-disjoint" clauses that has been studied e.g. in [16]. It has
similarly been observed previously that arbitrary recursion-free
clauses can be turned into CDD clauses by copying shared clauses, such
copying is used routinely in solvers; and it is known that such
duplication is a common bottleneck in practice (the theoretic
exponential blow-up caused by copying can indeed be observed), in
contrast to what is claimed in this submission. Horn solvers already
tend to apply various heuristics to keep the blow-up manageable.

The most interesting part of the paper is the bottom-up algorithm for
solving CDD clauses, section 4.3. The algorithm resembles existing
methods for tree interpolation [8], but can also handle disjunctions
(that essentially occur when multiple clauses with common head symbol
are solved) by directly including them in the generated binary
interpolation problems, and by introducing additional Boolean
flags. Not really ground-breaking, and rather similar to previous
algorithms for solving the same problem (e.g. in [16]), but the way
the algorithm is formulated here is quite neat and compact.

The section on experiments lacks basic information: how big are the
considered systems of clauses? how were the clauses extracted from
SV-COMP problems (by which the authors presumably mean C programs)?
how close are the systems to CDD? how many of the systems are
satisfiable/unsatisfiable? do any of the considered solvers time out
on some of the problems? Where can the clauses and the implementation
done by the authors be downloaded to reproduce the experiments?

Also, the time needed to solve CDD systems is relevant, but even more
important is the "quality" of obtained solutions for later model
checking. Do the solutions differ in size or complexity of Boolean
structure? Does the use of different solvers have an impact on the
runtime of the overall Duality solver?

Smaller comments:

p1: relational-predicate -> relational predicate

p2: logical-interpolation -> logical interpolation
    cast -> case

p4, second paragraph: the description how existing solvers handle
recursion-free clauses is a bit misleading. Most model checkers (based
on CEGAR and predicate abstraction) would not consider recursion-free
clauses corresponding to multiple derivations in the first place, they
would only look at a single derivation at a time (corresponding to one
CEGAR counterexample). As a result, they would not naively enumerate
exponentially many derivations either.

p5: the situation when interpolating arrays is a bit more complicated
than presented here

p6: R \in Preds -> should this be R \subseteq Preds? In general, the
notation introduced in section 3.2 is rather non-standard (at least in
this community) and cumbersome to read

p6: it should be made clear that bodies of clauses are always finite,
and that only finite systems of clauses are considered

p11: pre_R(C, ...) should be pre_R(S, ...)?

p12: what is Ctr[B], the constraint of a clause?